# On-policy Self-Distillation Fine-Tuning (SDFT) config
# Hyperparameters aligned with Shenfeld et al. (2025), arXiv:2601.19897

model:
  name: Qwen/Qwen3-4B-Thinking-2507
  attn_implementation: flash_attention_2

datasets:
  - name: lightonai/olmo_think_fr_sdft
    split: train
    column_map:
      prompt: query
      completion: demonstration_think
    # max_samples: 10000

validation:
  split_ratio: 0.01
  seed: 42

sdft:
  max_prompt_length: 1024
  max_completion_length: 2048          # paper: 2048 (skill learning)
  temperature: 1.0                     # paper: 1.0 with nucleus sampling
  kl_temperature: 1.0
  top_p: 0.95                          # paper: 0.95
  top_k: -1
  min_p: 0.0
  repetition_penalty: 1.0
  ema_alpha: 0.02                      # {0.01, 0.02, 0.05}
  mask_first_n_tokens: 5              # mask first few tokens to suppress demo artifacts
  steps_per_generation: 4
  disable_dropout: true
  vllm_gpu_memory_utilization: 0.4
  student_system_prompt: null
  teacher_system_prompt: null

training:
  output_dir: ./training_results/sdft_test
  per_device_train_batch_size: 4       
  gradient_accumulation_steps: 4
  num_train_epochs: 2          
  learning_rate: 1e-5                  # {5e-6, 1e-5, 5e-5}
  warmup_steps: 10           
  lr_scheduler_type: cosine
  weight_decay: 0.0              
  logging_steps: 1
  save_steps: 500
  bf16: true
  gradient_checkpointing: true
  max_grad_norm: 1.0

wandb:
  enabled: true
  project: EasyTraining
  run_name: sdft_test
