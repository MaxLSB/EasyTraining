# TRL SFT training config

model:
  name: Qwen/Qwen3-4B
  attn_implementation: flash_attention_2

datasets:
  - name: HuggingFaceH4/ultrachat_200k
    split: train_sft
    column_map:
      conversations: messages

validation:
  split_ratio: 0.01
  seed: 42

training:
  output_dir: ./outputs/sft_test
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4
  num_train_epochs: 1
  learning_rate: 2e-5
  warmup_ratio: 0.1
  lr_scheduler_type: cosine
  weight_decay: 0.01
  logging_steps: 1
  save_strategy: epoch
  eval_strategy: steps
  eval_steps: 100
  report_to: wandb
  bf16: true
  gradient_checkpointing: true
  optim: adamw_torch
  max_grad_norm: 1.0
  max_seq_length: 4096
  packing: true
  dataset_num_proc: 8

lora:
  enabled: true
  r: 32
  alpha: 64
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

eval_samples:
  enabled: true
  num_samples: 5
  max_new_tokens: 500

wandb:
  enabled: true
  project: EasyTraining
  run_name: sft_test
